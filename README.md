# Master Thesis repository - Deep Metrics Learning based Biometric Identification of Human Gait.

## Required
- The project requires Anaconda package manager, it can be installed from https://docs.anaconda.com/anaconda/install/

## Installation steps
- Go to the root directory where the conda environment file is present. Use the command, **conda env create -f environment.yml** to create the environment and have the necessary packages installed automatically.
- **Note that the training was done in CPU and NOT in GPU. So the environment created from the environment.yml file will not have tensorflow running on gpu. Please install the desirable tensorflow that supports GPU usage.**

## Lauch the notebook
- Type **conda activate environment_name** to activate the environment.
- Type **jupyter lab** to launch jupyterlab(this is similar to jupyter notebook but jupyterlab has file access bar which makes it easy to access files and folders.)

## Repository structure
There are two main directories in this repository.
- final_code/ => This contains the actual python files and the jupyter notebook that are required for implementing the deep metrics learning technique. The following sub-directories are present within the final_code/ dir:
    - data/ => the hdf5 data file that contains the 2D human joint coordinates for the gait sequences should be placed in this dir.
    - models/ => all the model binaries go into this dir. It is created automatically by the training script while saving the model checkpoints during training.
    - scripts/ => contains all the python files that supports the training notebook.
    - tensorboard_logdir => this directory is dedicated to save the checkpoint files for t-SNE visualization of the feature vectors generated by the model. The sub dirs present within this dir are used for different set of checkpoint files for various models.
    - Deep_Metrics_Learning.ipynb notebook is the main notebook that access various python files that helps in model training and inference.
    - Tensorboard.ipynb notebook is the notebook for generating the checkpoint files to visualize the feature vectors generated by the model using t-SNE algorithm.
- archives/ => These have additional files related to the thesis project and are only for additional references if required.

## Model Training
- Follow the instruction in the Deep_Metrics_Learning.ipynb notebook for step by step training and inference of the deep metrics learning technique. 
- There two types of triplet mining strategies implemented in the repository, Online mining and Offline mining. From the thesis experience, offline mining works well for this particular dataset.

## Model Evaluation
- The model evaluation is also present in the Deep_Metrics_Learning.ipynb notebook.
- In deep metrics learning, the original data is split into training set and the validation set. The split is based on the person ids, for example if there are 20 person ids from id 1, id 2, ... , id 19, id 20, then the training set might contain only those sequences belonging to the ids 1, 2, ... , 14, 15 and the validation set will contain those sequences from the rest of thebperson ids.
- The validation set is further split into the anchor/query set and the gallery set. Here, the split is in such a way the few of the sequences from each person id are put into the anchor set and the rest goes into the gallery set. For example, if the person id 16, 17, 18, 19 and 20 belongs to the validation set, the may be 20% of the sequences from each of the person ids are put into the anchor/query set and the rest goes into the gallery set.
- Once the model is trained, the feature vectors are generated for the gallery set of the validation set with the trained model. Then the feature vectors are generated for the sequences present in the anchor/query set and the nearest neighbors for each of them are calculated using the rank-k and the mean Average Precision evaluation metrics.

## Useful links
- The triplet loss is explain here --> [Triplet Loss](https://omoindrot.github.io/triplet-loss)
- Medium blog for Image similarity using triplet loss --> [Triplet Loss - Medium blog](https://towardsdatascience.com/image-similarity-using-triplet-loss-3744c0f67973)
- The concept of using triplets were first introduced in this paper --> [Facenet Paper](https://arxiv.org/abs/1503.03832)
